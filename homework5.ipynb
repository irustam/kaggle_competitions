{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing libraries\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "from catboost.utils import get_roc_curve\n",
    "from catboost import Pool\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shap\n",
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import  Image\n",
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns #visualization\n",
    "import featuretools as ft\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import io\n",
    "import plotly.offline as py#visualization\n",
    "py.init_notebook_mode(connected=True)#visualization\n",
    "import plotly.graph_objs as go#visualization\n",
    "import plotly.tools as tls#visualization\n",
    "import plotly.figure_factory as ff#visualization\n",
    "\n",
    "from featexp import get_univariate_plots\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold, StratifiedKFold, GroupKFold\n",
    "from scipy.stats import shapiro, probplot, ttest_ind, mannwhitneyu, chi2_contingency, ks_2samp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, scorer, log_loss\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, f1_score, cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder as le\n",
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 5\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 400)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(feature_names, feature_importances, get_top=None):\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "       \n",
    "    plt.figure(figsize = (20, len(feature_importances) * 0.5))\n",
    "    \n",
    "    sns.barplot(feature_importances['importance'], feature_importances['feature'])\n",
    "    \n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Importance of features')\n",
    "    plt.show()\n",
    "    \n",
    "    if get_top is not None:\n",
    "        return feature_importances['feature'][:get_top].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(**kwargs):\n",
    "    res_lst = []\n",
    "    \n",
    "    for k, v in kwargs.items():  \n",
    "        fpr, tpr, _ = roc_curve(v[0], v[1])\n",
    "        res_lst.append((fpr, tpr, k))\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    for res in res_lst:\n",
    "        plt.plot(res[0], res[1], label=res[2])\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return res_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape = 179977 rows, 398 cols\n",
      "train.shape = 100001 rows, 394 cols\n"
     ]
    }
   ],
   "source": [
    "#Загружаю частично обработанный мною от пропусков в ДЗ2 датасет:\n",
    "# train = pd.read_csv(\"assignment2_data/assignment_2_train.csv\")\n",
    "train = pd.read_csv(\"assignment2_data/train2.csv\")\n",
    "lb = pd.read_csv(\"assignment2_data/assignment_2_test.csv\")\n",
    "\n",
    "print(\"train.shape = {} rows, {} cols\".format(*train.shape))\n",
    "print(\"train.shape = {} rows, {} cols\".format(*lb.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 18, 'isFraud')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "num_features = train.select_dtypes(\"number\").columns.to_list()\n",
    "dum_features = train.select_dtypes(\"object\").columns.to_list()\n",
    "target = num_features.pop(1)\n",
    "len(num_features), len(dum_features), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cross_validation(X: pd.DataFrame,\n",
    "                          y: pd.Series,\n",
    "                          estimator: object,\n",
    "                          metric: callable,\n",
    "                          cv_strategy,\n",
    "                          params,\n",
    "                          groups: pd.Series = pd.Series()):\n",
    "    \"\"\"\n",
    "    Кросс-валидация.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame\n",
    "        Матрица признаков.\n",
    "\n",
    "    y: pd.Series\n",
    "        Вектор целевой переменной.\n",
    "\n",
    "    estimator: callable\n",
    "        Объект модели для обучения.\n",
    "\n",
    "    metric: callable\n",
    "        Метрика для оценки качества решения.\n",
    "        Ожидается, что на вход будет передана функция,\n",
    "        которая принимает 2 аргумента: y_true, y_pred.\n",
    "\n",
    "    cv_strategy: cross-validation generator\n",
    "        Объект для описания стратегии кросс-валидации.\n",
    "        Ожидается, что на вход будет передан объект типа\n",
    "        KFold или StratifiedKFold.\n",
    "    groups:\n",
    "        Если в cv_strategy передаем GroupKFold, то нужно передать groups,\n",
    "        чтобы разделять по этим данным датасет на группы\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    oof_score: float\n",
    "        Значение метрики качества на OOF-прогнозах.\n",
    "\n",
    "    fold_train_scores: List[float]\n",
    "        Значение метрики качества на каждом обучающем датасете кросс-валидации.\n",
    "\n",
    "    fold_valid_scores: List[float]\n",
    "        Значение метрики качества на каждом валидационном датасете кросс-валидации.\n",
    "\n",
    "    oof_predictions: np.array\n",
    "        Прогнозы на OOF.\n",
    "\n",
    "    \"\"\"\n",
    "    estimators, fold_train_scores, fold_valid_scores = [], [], []\n",
    "    oof_predictions = np.zeros(X.shape[0])\n",
    "    if len(groups) > 0:\n",
    "        cv_generator = cv_strategy.split(X, y, groups)\n",
    "    else:\n",
    "        cv_generator = cv_strategy.split(X, y)\n",
    "\n",
    "    for fold_number, (train_idx, valid_idx) in enumerate(cv_generator):\n",
    "        x_train, x_valid = X.loc[train_idx], X.loc[valid_idx]\n",
    "        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
    "        \n",
    "        model_train = estimator.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=50,\n",
    "            evals=[(dtrain, \"train\"), (dvalid, \"valid\")],\n",
    "            verbose_eval=False,\n",
    "            maximize=True,\n",
    "        )\n",
    "        y_train_pred = model_train.predict(dtrain)\n",
    "        y_valid_pred = model_train.predict(dvalid)\n",
    "\n",
    "        fold_train_scores.append(metric(y_train, y_train_pred))\n",
    "        fold_valid_scores.append(metric(y_valid, y_valid_pred))\n",
    "        oof_predictions[valid_idx] = y_valid_pred\n",
    "\n",
    "        msg = (\n",
    "            f\"Fold: {fold_number+1}, train-observations = {len(train_idx)}, \"\n",
    "            f\"valid-observations = {len(valid_idx)}\\n\"\n",
    "            f\"train-score = {round(fold_train_scores[fold_number], 4)}, \"\n",
    "            f\"valid-score = {round(fold_valid_scores[fold_number], 4)}\" \n",
    "        )\n",
    "        print(msg)\n",
    "        print(\"=\"*69)\n",
    "        estimators.append(model_train)\n",
    "\n",
    "    oof_score = metric(y, oof_predictions)\n",
    "    print(f\"CV-results train: {round(np.mean(fold_train_scores), 4)} +/- {round(np.std(fold_train_scores), 3)}\")\n",
    "    print(f\"CV-results valid: {round(np.mean(fold_valid_scores), 4)} +/- {round(np.std(fold_valid_scores), 3)}\")\n",
    "    print(f\"OOF-score = {round(oof_score, 4)}\")\n",
    "\n",
    "    return estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 0:__ выбрать любую модель машнного обучения и зафиксировать любой тип валидации. Обучить базовую модель и зафиксировать базовое качество модели. В каждом следующем задании нужно будет обучить выбранную модель и оценивать ее качество на зафиксированной схеме валидации. После каждого задания, требуется сделать вывод о достигаемом качестве модели, по сравнению с качестом из предыдущего шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаю валидацию GroupKFold, которая показала самые надежные метрики качества в 3-м ДЗ: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card1</th>\n",
       "      <th>card4</th>\n",
       "      <th>card6</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>visa</td>\n",
       "      <td>debit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>visa</td>\n",
       "      <td>credit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card1 card4   card6  user_id\n",
       "0   1001  visa   debit        0\n",
       "1   1004  visa  credit        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_cols = ['card1', 'card4', 'card6']\n",
    "all_groupby = train.groupby(card_cols, as_index=False).agg({'TransactionID': 'count'})\n",
    "all_groupby['user_id'] = all_groupby.index\n",
    "all_groupby.drop('TransactionID', axis=1, inplace=True)\n",
    "all_groupby.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.merge(train, all_groupby, on=card_cols, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка лидерборда для получения оценок метрики качества на нем:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавляю новые признаки в датасет лидерборда для получения результатов на нем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape = 100001 rows, 398 cols\n"
     ]
    }
   ],
   "source": [
    "lb.loc[lb['P_emaildomain']=='gmail', 'P_emaildomain'] = 'gmail.com'\n",
    "lb.loc[lb['R_emaildomain']=='gmail', 'R_emaildomain'] = 'gmail.com'\n",
    "lb['P_emaildomain'].fillna('unknown', inplace=True)\n",
    "lb['R_emaildomain'].fillna('unknown', inplace=True)\n",
    "\n",
    "lb['P_emaildomain_zone'] = lb['P_emaildomain'].apply(\n",
    "    lambda x: '.'.join(x.split('.')[1:]) if x!='unknown' else 'unknown'\n",
    ")\n",
    "lb['R_emaildomain_zone'] = lb['R_emaildomain'].apply(\n",
    "    lambda x: '.'.join(x.split('.')[1:]) if x!='unknown' else 'unknown'\n",
    ")\n",
    "lb['P_emaildomain_domain'] = lb['P_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "lb['R_emaildomain_domain'] = lb['R_emaildomain'].apply(lambda x: x.split('.')[0])\n",
    "print(\"train.shape = {} rows, {} cols\".format(*lb.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductCD               0\n",
       "card4                   0\n",
       "card6                   0\n",
       "P_emaildomain           0\n",
       "R_emaildomain           0\n",
       "M1                      0\n",
       "M2                      0\n",
       "M3                      0\n",
       "M4                      0\n",
       "M5                      0\n",
       "M6                      0\n",
       "M7                      0\n",
       "M8                      0\n",
       "M9                      0\n",
       "P_emaildomain_zone      0\n",
       "R_emaildomain_zone      0\n",
       "P_emaildomain_domain    0\n",
       "R_emaildomain_domain    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dum_na = lb[dum_features].isnull().sum()\n",
    "col_na_lst = dum_na.loc[dum_na > 0].index.tolist()\n",
    "lb[col_na_lst] = lb[col_na_lst].fillna('unknown')\n",
    "lb[dum_features].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_groupby = lb.groupby(card_cols, as_index=False).agg({'TransactionID': 'count'})\n",
    "all_groupby['user_id'] = all_groupby.index\n",
    "all_groupby.drop('TransactionID', axis=1, inplace=True)\n",
    "lb_train_df = pd.merge(lb, all_groupby, on=card_cols, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет лидерборда для моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lb = lb_train_df[target]\n",
    "lb_train = lb_train_df[num_features + dum_features + ['user_id']].apply(le().fit_transform)\n",
    "dleaderboard = xgb.DMatrix(\n",
    "    data=lb_train, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кроссвалидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train2[num_features + dum_features + ['user_id']].apply(le().fit_transform)\n",
    "y = train2[target]\n",
    "groups = train2['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_strategy = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.1,\n",
    "#     \"n_estimators\": 1000,\n",
    "    \"reg_lambda\": 100,\n",
    "    \"max_depth\": 4,\n",
    "    \"gamma\": 10,\n",
    "    \"nthread\": 6,\n",
    "    \"seed\": 27\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9227, valid-score = 0.8886\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9258, valid-score = 0.88\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9198, valid-score = 0.8916\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9189, valid-score = 0.8908\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.923, valid-score = 0.8857\n",
      "=====================================================================\n",
      "CV-results train: 0.922 +/- 0.002\n",
      "CV-results valid: 0.8873 +/- 0.004\n",
      "OOF-score = 0.8876\n",
      "Wall time: 18min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators, oof_score, fold_train_scores, fold_valid_scores, oof_predictions = make_cross_validation(\n",
    "    data, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8581529734586558, 0.8599733635128151, 0.8593279332029686, 0.855988854179111, 0.8585331642380771]\n",
      "Leaderboard score 0.8584 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst2 = []\n",
    "for est in estimators:\n",
    "    y_lb_pred = est.predict(dleaderboard)\n",
    "    lb_roc_auc_lst2.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst2)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst2), 4),  '+/-', round(np.std(lb_roc_auc_lst2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду ниже, чем дает кросс-валидация по valid фолдам и по OOF\n",
    "- Но результат очень стабильный - разброс всего 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1:__ признак TransactionDT - это смещение в секундах относительно базовой даты. Базовая дата - 2017-12-01, преобразовать признак TransactionDT в datetime, прибавив к базовой дате исходное значение признака. Из полученного признака выделить год, месяц, день недели, час, день."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>addr1</th>\n",
       "      <th>addr2</th>\n",
       "      <th>dist1</th>\n",
       "      <th>dist2</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>D10</th>\n",
       "      <th>D11</th>\n",
       "      <th>D12</th>\n",
       "      <th>D13</th>\n",
       "      <th>D14</th>\n",
       "      <th>D15</th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>M4</th>\n",
       "      <th>M5</th>\n",
       "      <th>M6</th>\n",
       "      <th>M7</th>\n",
       "      <th>M8</th>\n",
       "      <th>M9</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>V40</th>\n",
       "      <th>V41</th>\n",
       "      <th>V42</th>\n",
       "      <th>V43</th>\n",
       "      <th>V44</th>\n",
       "      <th>V45</th>\n",
       "      <th>V46</th>\n",
       "      <th>V47</th>\n",
       "      <th>V48</th>\n",
       "      <th>V49</th>\n",
       "      <th>V50</th>\n",
       "      <th>V51</th>\n",
       "      <th>V52</th>\n",
       "      <th>V53</th>\n",
       "      <th>V54</th>\n",
       "      <th>V55</th>\n",
       "      <th>V56</th>\n",
       "      <th>V57</th>\n",
       "      <th>V58</th>\n",
       "      <th>V59</th>\n",
       "      <th>V60</th>\n",
       "      <th>V61</th>\n",
       "      <th>V62</th>\n",
       "      <th>V63</th>\n",
       "      <th>V64</th>\n",
       "      <th>V65</th>\n",
       "      <th>V66</th>\n",
       "      <th>V67</th>\n",
       "      <th>V68</th>\n",
       "      <th>V69</th>\n",
       "      <th>V70</th>\n",
       "      <th>V71</th>\n",
       "      <th>V72</th>\n",
       "      <th>V73</th>\n",
       "      <th>V74</th>\n",
       "      <th>V75</th>\n",
       "      <th>V76</th>\n",
       "      <th>V77</th>\n",
       "      <th>V78</th>\n",
       "      <th>V79</th>\n",
       "      <th>V80</th>\n",
       "      <th>V81</th>\n",
       "      <th>V82</th>\n",
       "      <th>V83</th>\n",
       "      <th>V84</th>\n",
       "      <th>V85</th>\n",
       "      <th>V86</th>\n",
       "      <th>V87</th>\n",
       "      <th>V88</th>\n",
       "      <th>V89</th>\n",
       "      <th>V90</th>\n",
       "      <th>V91</th>\n",
       "      <th>V92</th>\n",
       "      <th>V93</th>\n",
       "      <th>V94</th>\n",
       "      <th>V95</th>\n",
       "      <th>V96</th>\n",
       "      <th>V97</th>\n",
       "      <th>V98</th>\n",
       "      <th>V99</th>\n",
       "      <th>V100</th>\n",
       "      <th>V101</th>\n",
       "      <th>V102</th>\n",
       "      <th>V103</th>\n",
       "      <th>V104</th>\n",
       "      <th>V105</th>\n",
       "      <th>V106</th>\n",
       "      <th>V107</th>\n",
       "      <th>V108</th>\n",
       "      <th>V109</th>\n",
       "      <th>V110</th>\n",
       "      <th>V111</th>\n",
       "      <th>V112</th>\n",
       "      <th>V113</th>\n",
       "      <th>V114</th>\n",
       "      <th>V115</th>\n",
       "      <th>V116</th>\n",
       "      <th>V117</th>\n",
       "      <th>V118</th>\n",
       "      <th>V119</th>\n",
       "      <th>V120</th>\n",
       "      <th>V121</th>\n",
       "      <th>V122</th>\n",
       "      <th>V123</th>\n",
       "      <th>V124</th>\n",
       "      <th>V125</th>\n",
       "      <th>V126</th>\n",
       "      <th>V127</th>\n",
       "      <th>V128</th>\n",
       "      <th>V129</th>\n",
       "      <th>V130</th>\n",
       "      <th>V131</th>\n",
       "      <th>V132</th>\n",
       "      <th>V133</th>\n",
       "      <th>V134</th>\n",
       "      <th>V135</th>\n",
       "      <th>V136</th>\n",
       "      <th>V137</th>\n",
       "      <th>V138</th>\n",
       "      <th>V139</th>\n",
       "      <th>V140</th>\n",
       "      <th>V141</th>\n",
       "      <th>V142</th>\n",
       "      <th>V143</th>\n",
       "      <th>V144</th>\n",
       "      <th>V145</th>\n",
       "      <th>V146</th>\n",
       "      <th>V147</th>\n",
       "      <th>V148</th>\n",
       "      <th>V149</th>\n",
       "      <th>V150</th>\n",
       "      <th>V151</th>\n",
       "      <th>V152</th>\n",
       "      <th>V153</th>\n",
       "      <th>V154</th>\n",
       "      <th>V155</th>\n",
       "      <th>V156</th>\n",
       "      <th>V157</th>\n",
       "      <th>V158</th>\n",
       "      <th>V159</th>\n",
       "      <th>V160</th>\n",
       "      <th>V161</th>\n",
       "      <th>V162</th>\n",
       "      <th>V163</th>\n",
       "      <th>V164</th>\n",
       "      <th>V165</th>\n",
       "      <th>V166</th>\n",
       "      <th>V167</th>\n",
       "      <th>V168</th>\n",
       "      <th>V169</th>\n",
       "      <th>V170</th>\n",
       "      <th>V171</th>\n",
       "      <th>V172</th>\n",
       "      <th>V173</th>\n",
       "      <th>V174</th>\n",
       "      <th>V175</th>\n",
       "      <th>V176</th>\n",
       "      <th>V177</th>\n",
       "      <th>V178</th>\n",
       "      <th>V179</th>\n",
       "      <th>V180</th>\n",
       "      <th>V181</th>\n",
       "      <th>V182</th>\n",
       "      <th>V183</th>\n",
       "      <th>V184</th>\n",
       "      <th>V185</th>\n",
       "      <th>V186</th>\n",
       "      <th>V187</th>\n",
       "      <th>V188</th>\n",
       "      <th>V189</th>\n",
       "      <th>V190</th>\n",
       "      <th>V191</th>\n",
       "      <th>V192</th>\n",
       "      <th>V193</th>\n",
       "      <th>V194</th>\n",
       "      <th>V195</th>\n",
       "      <th>V196</th>\n",
       "      <th>V197</th>\n",
       "      <th>V198</th>\n",
       "      <th>V199</th>\n",
       "      <th>V200</th>\n",
       "      <th>V201</th>\n",
       "      <th>V202</th>\n",
       "      <th>V203</th>\n",
       "      <th>V204</th>\n",
       "      <th>V205</th>\n",
       "      <th>V206</th>\n",
       "      <th>V207</th>\n",
       "      <th>V208</th>\n",
       "      <th>V209</th>\n",
       "      <th>V210</th>\n",
       "      <th>V211</th>\n",
       "      <th>V212</th>\n",
       "      <th>V213</th>\n",
       "      <th>V214</th>\n",
       "      <th>V215</th>\n",
       "      <th>V216</th>\n",
       "      <th>V217</th>\n",
       "      <th>V218</th>\n",
       "      <th>V219</th>\n",
       "      <th>V220</th>\n",
       "      <th>V221</th>\n",
       "      <th>V222</th>\n",
       "      <th>V223</th>\n",
       "      <th>V224</th>\n",
       "      <th>V225</th>\n",
       "      <th>V226</th>\n",
       "      <th>V227</th>\n",
       "      <th>V228</th>\n",
       "      <th>V229</th>\n",
       "      <th>V230</th>\n",
       "      <th>V231</th>\n",
       "      <th>V232</th>\n",
       "      <th>V233</th>\n",
       "      <th>V234</th>\n",
       "      <th>V235</th>\n",
       "      <th>V236</th>\n",
       "      <th>V237</th>\n",
       "      <th>V238</th>\n",
       "      <th>V239</th>\n",
       "      <th>V240</th>\n",
       "      <th>V241</th>\n",
       "      <th>V242</th>\n",
       "      <th>V243</th>\n",
       "      <th>V244</th>\n",
       "      <th>V245</th>\n",
       "      <th>V246</th>\n",
       "      <th>V247</th>\n",
       "      <th>V248</th>\n",
       "      <th>V249</th>\n",
       "      <th>V250</th>\n",
       "      <th>V251</th>\n",
       "      <th>V252</th>\n",
       "      <th>V253</th>\n",
       "      <th>V254</th>\n",
       "      <th>V255</th>\n",
       "      <th>V256</th>\n",
       "      <th>V257</th>\n",
       "      <th>V258</th>\n",
       "      <th>V259</th>\n",
       "      <th>V260</th>\n",
       "      <th>V261</th>\n",
       "      <th>V262</th>\n",
       "      <th>V263</th>\n",
       "      <th>V264</th>\n",
       "      <th>V265</th>\n",
       "      <th>V266</th>\n",
       "      <th>V267</th>\n",
       "      <th>V268</th>\n",
       "      <th>V269</th>\n",
       "      <th>V270</th>\n",
       "      <th>V271</th>\n",
       "      <th>V272</th>\n",
       "      <th>V273</th>\n",
       "      <th>V274</th>\n",
       "      <th>V275</th>\n",
       "      <th>V276</th>\n",
       "      <th>V277</th>\n",
       "      <th>V278</th>\n",
       "      <th>V279</th>\n",
       "      <th>V280</th>\n",
       "      <th>V281</th>\n",
       "      <th>V282</th>\n",
       "      <th>V283</th>\n",
       "      <th>V284</th>\n",
       "      <th>V285</th>\n",
       "      <th>V286</th>\n",
       "      <th>V287</th>\n",
       "      <th>V288</th>\n",
       "      <th>V289</th>\n",
       "      <th>V290</th>\n",
       "      <th>V291</th>\n",
       "      <th>V292</th>\n",
       "      <th>V293</th>\n",
       "      <th>V294</th>\n",
       "      <th>V295</th>\n",
       "      <th>V296</th>\n",
       "      <th>V297</th>\n",
       "      <th>V298</th>\n",
       "      <th>V299</th>\n",
       "      <th>V300</th>\n",
       "      <th>V301</th>\n",
       "      <th>V302</th>\n",
       "      <th>V303</th>\n",
       "      <th>V304</th>\n",
       "      <th>V305</th>\n",
       "      <th>V306</th>\n",
       "      <th>V307</th>\n",
       "      <th>V308</th>\n",
       "      <th>V309</th>\n",
       "      <th>V310</th>\n",
       "      <th>V311</th>\n",
       "      <th>V312</th>\n",
       "      <th>V313</th>\n",
       "      <th>V314</th>\n",
       "      <th>V315</th>\n",
       "      <th>V316</th>\n",
       "      <th>V317</th>\n",
       "      <th>V318</th>\n",
       "      <th>V319</th>\n",
       "      <th>V320</th>\n",
       "      <th>V321</th>\n",
       "      <th>V322</th>\n",
       "      <th>V323</th>\n",
       "      <th>V324</th>\n",
       "      <th>V325</th>\n",
       "      <th>V326</th>\n",
       "      <th>V327</th>\n",
       "      <th>V328</th>\n",
       "      <th>V329</th>\n",
       "      <th>V330</th>\n",
       "      <th>V331</th>\n",
       "      <th>V332</th>\n",
       "      <th>V333</th>\n",
       "      <th>V334</th>\n",
       "      <th>V335</th>\n",
       "      <th>V336</th>\n",
       "      <th>V337</th>\n",
       "      <th>V338</th>\n",
       "      <th>V339</th>\n",
       "      <th>P_emaildomain_zone</th>\n",
       "      <th>R_emaildomain_zone</th>\n",
       "      <th>P_emaildomain_domain</th>\n",
       "      <th>R_emaildomain_domain</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>315.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>M2</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>7077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>credit</td>\n",
       "      <td>325.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>M0</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>com</td>\n",
       "      <td>unknown</td>\n",
       "      <td>gmail</td>\n",
       "      <td>unknown</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "\n",
       "   card2  card3       card4  card5   card6  addr1  addr2  dist1  dist2  \\\n",
       "0    NaN  150.0    discover  142.0  credit  315.0   87.0   19.0    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  credit  325.0   87.0    NaN    NaN   \n",
       "\n",
       "  P_emaildomain R_emaildomain   C1   C2   C3   C4   C5   C6   C7   C8   C9  \\\n",
       "0       unknown       unknown  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0   \n",
       "1     gmail.com       unknown  1.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n",
       "\n",
       "   C10  C11  C12  C13  C14    D1  D2    D3   D4  D5  D6  D7  D8  D9   D10  \\\n",
       "0  0.0  2.0  0.0  1.0  1.0  14.0 NaN  13.0  NaN NaN NaN NaN NaN NaN  13.0   \n",
       "1  0.0  1.0  0.0  1.0  1.0   0.0 NaN   NaN  0.0 NaN NaN NaN NaN NaN   0.0   \n",
       "\n",
       "    D11  D12  D13  D14  D15       M1       M2       M3  M4 M5 M6       M7  \\\n",
       "0  13.0  NaN  NaN  NaN  0.0        T        T        T  M2  F  T  unknown   \n",
       "1   NaN  NaN  NaN  NaN  0.0  unknown  unknown  unknown  M0  T  T  unknown   \n",
       "\n",
       "        M8       M9   V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  V11  \\\n",
       "0  unknown  unknown  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0   \n",
       "1  unknown  unknown  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "   V12  V13  V14  V15  V16  V17  V18  V19  V20  V21  V22  V23  V24  V25  V26  \\\n",
       "0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0   \n",
       "1  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  1.0   \n",
       "\n",
       "   V27  V28  V29  V30  V31  V32  V33  V34  V35  V36  V37  V38  V39  V40  V41  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "   V42  V43  V44  V45  V46  V47  V48  V49  V50  V51  V52  V53  V54  V55  V56  \\\n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0   \n",
       "1  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0   \n",
       "\n",
       "   V57  V58  V59  V60  V61  V62  V63  V64  V65  V66  V67  V68  V69  V70  V71  \\\n",
       "0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   V72  V73  V74  V75  V76  V77  V78  V79  V80  V81  V82  V83  V84  V85  V86  \\\n",
       "0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0   \n",
       "\n",
       "   V87  V88  V89  V90  V91  V92  V93  V94  V95  V96  V97  V98  V99  V100  \\\n",
       "0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   0.0   \n",
       "1  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0   \n",
       "\n",
       "   V101  V102  V103  V104  V105  V106  V107  V108  V109  V110  V111  V112  \\\n",
       "0   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "   V113  V114  V115  V116  V117  V118  V119  V120  V121  V122  V123  V124  \\\n",
       "0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "1   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
       "\n",
       "   V125  V126   V127  V128  V129  V130  V131  V132   V133  V134  V135  V136  \\\n",
       "0   1.0   0.0  117.0   0.0   0.0   0.0   0.0   0.0  117.0   0.0   0.0   0.0   \n",
       "1   1.0   0.0    0.0   0.0   0.0   0.0   0.0   0.0    0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V137  V138  V139  V140  V141  V142  V143  V144  V145  V146  V147  V148  \\\n",
       "0   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V149  V150  V151  V152  V153  V154  V155  V156  V157  V158  V159  V160  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V161  V162  V163  V164  V165  V166  V167  V168  V169  V170  V171  V172  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V173  V174  V175  V176  V177  V178  V179  V180  V181  V182  V183  V184  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V185  V186  V187  V188  V189  V190  V191  V192  V193  V194  V195  V196  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V197  V198  V199  V200  V201  V202  V203  V204  V205  V206  V207  V208  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V209  V210  V211  V212  V213  V214  V215  V216  V217  V218  V219  V220  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V221  V222  V223  V224  V225  V226  V227  V228  V229  V230  V231  V232  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V233  V234  V235  V236  V237  V238  V239  V240  V241  V242  V243  V244  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V245  V246  V247  V248  V249  V250  V251  V252  V253  V254  V255  V256  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V257  V258  V259  V260  V261  V262  V263  V264  V265  V266  V267  V268  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V269  V270  V271  V272  V273  V274  V275  V276  V277  V278  V279  V280  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   0.0   0.0   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   0.0   0.0   \n",
       "\n",
       "   V281  V282  V283  V284  V285  V286  V287  V288  V289  V290  V291  V292  \\\n",
       "0   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   \n",
       "1   0.0   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   1.0   1.0   \n",
       "\n",
       "   V293  V294  V295  V296  V297  V298  V299  V300  V301  V302  V303  V304  \\\n",
       "0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   V305  V306   V307  V308  V309  V310  V311  V312  V313  V314  V315  V316  \\\n",
       "0   1.0   0.0  117.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1   1.0   0.0    0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "    V317  V318  V319  V320  V321  V322  V323  V324  V325  V326  V327  V328  \\\n",
       "0  117.0   0.0   0.0   0.0   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1    0.0   0.0   0.0   0.0   0.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "   V329  V330  V331  V332  V333  V334  V335  V336  V337  V338  V339  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "  P_emaildomain_zone R_emaildomain_zone P_emaildomain_domain  \\\n",
       "0            unknown            unknown              unknown   \n",
       "1                com            unknown                gmail   \n",
       "\n",
       "  R_emaildomain_domain  user_id  \n",
       "0              unknown     7077  \n",
       "1              unknown      957  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "basesec = datetime(2017,12,1).timestamp()\n",
    "train2['TransactionDT_date'] = train2['TransactionDT'].apply(lambda x: datetime.fromtimestamp(basesec+x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2['TransactionDT_year'] = train2['TransactionDT_date'].dt.year\n",
    "train2['TransactionDT_month'] = train2['TransactionDT_date'].dt.month\n",
    "train2['TransactionDT_day'] = train2['TransactionDT_date'].dt.day\n",
    "train2['TransactionDT_hour'] = train2['TransactionDT_date'].dt.hour\n",
    "train2['TransactionDT_dayofweek'] = train2['TransactionDT_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 18, 'isFraud')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train2_num_features = train2.select_dtypes(\"number\").columns.to_list()\n",
    "train2_dum_features = train2.select_dtypes(\"object\").columns.to_list()\n",
    "train2_target = train2_num_features.pop(1)\n",
    "len(train2_num_features), len(train2_dum_features), train2_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = train2[train2_num_features + train2_dum_features].apply(le().fit_transform)\n",
    "y = train2[target]\n",
    "groups = train2['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df2 = lb_train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df2['TransactionDT_date'] = lb_train_df2['TransactionDT'].apply(lambda x: datetime.fromtimestamp(basesec+x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df2['TransactionDT_year'] = lb_train_df2['TransactionDT_date'].dt.year\n",
    "lb_train_df2['TransactionDT_month'] = lb_train_df2['TransactionDT_date'].dt.month\n",
    "lb_train_df2['TransactionDT_day'] = lb_train_df2['TransactionDT_date'].dt.day\n",
    "lb_train_df2['TransactionDT_hour'] = lb_train_df2['TransactionDT_date'].dt.hour\n",
    "lb_train_df2['TransactionDT_dayofweek'] = lb_train_df2['TransactionDT_date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train2 = lb_train_df2[train2_num_features + train2_dum_features].apply(le().fit_transform)\n",
    "dleaderboard2 = xgb.DMatrix(\n",
    "    data=lb_train2, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9216, valid-score = 0.8867\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9248, valid-score = 0.8796\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9222, valid-score = 0.8905\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9193, valid-score = 0.8903\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9226, valid-score = 0.8862\n",
      "=====================================================================\n",
      "CV-results train: 0.9221 +/- 0.002\n",
      "CV-results valid: 0.8867 +/- 0.004\n",
      "OOF-score = 0.8869\n",
      "Wall time: 18min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators2, oof_score2, fold_train_scores2, fold_valid_scores2, oof_predictions2 = make_cross_validation(\n",
    "    data2, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8583522911552415, 0.8594135676408954, 0.8574208060511822, 0.8563526594627107, 0.8594577368818831]\n",
      "Leaderboard score 0.8582 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst2 = []\n",
    "for est in estimators2:\n",
    "    y_lb_pred = est.predict(dleaderboard2)\n",
    "    lb_roc_auc_lst2.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst2)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst2), 4),  '+/-', round(np.std(lb_roc_auc_lst2), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду незначительно ухудшился - на 0.002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2:__ сделать конкатенацию признаков\n",
    "* card1 + card2;\n",
    "* card1 + card2 + card3 + card5;\n",
    "* card1 + card2 + card3 + card5 + addr1 + addr2\n",
    "\n",
    "Рассматривать их как категориальных признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card1        0\n",
       "card2     2603\n",
       "card3        0\n",
       "card5      938\n",
       "addr1    19429\n",
       "addr2    19429\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Кол-во пропусков в данных:\n",
    "col_lst = ['card1', 'card2', 'card3', 'card5', 'addr1', 'addr2']\n",
    "train2[col_lst].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card1    9488\n",
       "card2     499\n",
       "card3      88\n",
       "card5      94\n",
       "addr1     269\n",
       "addr2      54\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Кол-во уникальных значений в данных\n",
    "train2[col_lst].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Заполним пропуски данных значением -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card1    0\n",
       "card2    0\n",
       "card3    0\n",
       "card5    0\n",
       "addr1    0\n",
       "addr2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train3 = train2.copy()\n",
    "train3[col_lst] = train3[col_lst].fillna(-999)\n",
    "train3[col_lst].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3['cards1'] = train3['card1'].astype(str) + train3['card2'].astype(str)\n",
    "train3['cards2'] = train3['card1'].astype(str) + train3['card2'].astype(str) + train3[\n",
    "    'card3'\n",
    "].astype(str) + train3['card5'].astype(str)\n",
    "train3['cards3'] = train3['card1'].astype(str) + train3['card2'].astype(str) + train3[\n",
    "    'card3'\n",
    "].astype(str) + train3['card5'].astype(str) + train3['addr1'].astype(str) + train3['addr2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 21, 'isFraud')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train3_num_features = train3.select_dtypes(\"number\").columns.to_list()\n",
    "train3_dum_features = train3.select_dtypes(\"object\").columns.to_list()\n",
    "train3_target = train3_num_features.pop(1)\n",
    "len(train3_num_features), len(train3_dum_features), train3_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = train3[train3_num_features + train3_dum_features].apply(le().fit_transform)\n",
    "y = train3[target]\n",
    "groups = train3['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df3 = lb_train_df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df3[col_lst] = lb_train_df3[col_lst].fillna(-999)\n",
    "lb_train_df3['cards1'] = lb_train_df3['card1'].astype(str) + lb_train_df3['card2'].astype(str)\n",
    "lb_train_df3['cards2'] = lb_train_df3['card1'].astype(str) + lb_train_df3['card2'].astype(str) + lb_train_df3[\n",
    "    'card3'\n",
    "].astype(str) + lb_train_df3['card5'].astype(str)\n",
    "lb_train_df3['cards3'] = lb_train_df3['card1'].astype(str) + lb_train_df3['card2'].astype(str) + lb_train_df3[\n",
    "    'card3'\n",
    "].astype(str) + lb_train_df3['card5'].astype(str) + lb_train_df3['addr1'].astype(str) + lb_train_df3['addr2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card1    0\n",
       "card2    0\n",
       "card3    0\n",
       "card5    0\n",
       "addr1    0\n",
       "addr2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_train_df3[col_lst].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train3 = lb_train_df3[train3_num_features + train3_dum_features].apply(le().fit_transform)\n",
    "dleaderboard3 = xgb.DMatrix(\n",
    "    data=lb_train3, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9218, valid-score = 0.8886\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9282, valid-score = 0.8821\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9247, valid-score = 0.8931\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9241, valid-score = 0.8937\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9236, valid-score = 0.8848\n",
      "=====================================================================\n",
      "CV-results train: 0.9245 +/- 0.002\n",
      "CV-results valid: 0.8885 +/- 0.005\n",
      "OOF-score = 0.8886\n",
      "Wall time: 20min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators3, oof_score3, fold_train_scores3, fold_valid_scores3, oof_predictions3 = make_cross_validation(\n",
    "    data3, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8607876278291391, 0.8599207870039998, 0.8593882419933762, 0.8577212733045283, 0.8590504564726956]\n",
      "Leaderboard score 0.8594 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst3 = []\n",
    "for est in estimators3:\n",
    "    y_lb_pred = est.predict(dleaderboard3)\n",
    "    lb_roc_auc_lst3.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst3)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst3), 4),  '+/-', round(np.std(lb_roc_auc_lst3), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду стал лучше +0.0012 с сохранением разброса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 3:__ Сделать FrequencyEncoder для признаков card1 - card6, addr1, addr2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card1    0\n",
       "card2    0\n",
       "card3    0\n",
       "card4    0\n",
       "card5    0\n",
       "card6    0\n",
       "addr1    0\n",
       "addr2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_lst = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2']\n",
    "train3[col_lst].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4 = train3.copy()\n",
    "for col in col_lst:\n",
    "    freq_encoder = train4[col].value_counts(normalize=True)\n",
    "    train4[f\"{col}_freq_enc\"] = train4[col].map(freq_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393, 21, 'isFraud')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train4_num_features = train4.select_dtypes(\"number\").columns.to_list()\n",
    "train4_dum_features = train4.select_dtypes(\"object\").columns.to_list()\n",
    "train4_target = train4_num_features.pop(1)\n",
    "len(train4_num_features), len(train4_dum_features), train4_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = train4[train4_num_features + train4_dum_features].apply(le().fit_transform)\n",
    "y = train4[target]\n",
    "groups = train4['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card1    0\n",
       "card2    0\n",
       "card3    0\n",
       "card4    0\n",
       "card5    0\n",
       "card6    0\n",
       "addr1    0\n",
       "addr2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_train_df4 = lb_train_df3.copy()\n",
    "lb_train_df4[col_lst].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_lst:\n",
    "    freq_encoder = lb_train_df4[col].value_counts(normalize=True)\n",
    "    lb_train_df4[f\"{col}_freq_enc\"] = lb_train_df4[col].map(freq_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train4 = lb_train_df4[train4_num_features + train4_dum_features].apply(le().fit_transform)\n",
    "dleaderboard4 = xgb.DMatrix(\n",
    "    data=lb_train4, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9267, valid-score = 0.8862\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9285, valid-score = 0.8813\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9234, valid-score = 0.8911\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9209, valid-score = 0.8933\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9236, valid-score = 0.8903\n",
      "=====================================================================\n",
      "CV-results train: 0.9246 +/- 0.003\n",
      "CV-results valid: 0.8884 +/- 0.004\n",
      "OOF-score = 0.8885\n",
      "Wall time: 18min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators4, oof_score4, fold_train_scores4, fold_valid_scores4, oof_predictions4 = make_cross_validation(\n",
    "    data4, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8608607258324295, 0.8585465273258968, 0.8562135330488072, 0.8575282532021921, 0.8539724912994274]\n",
      "Leaderboard score 0.8574 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst4 = []\n",
    "for est in estimators4:\n",
    "    y_lb_pred = est.predict(dleaderboard4)\n",
    "    lb_roc_auc_lst4.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst4)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst4), 4),  '+/-', round(np.std(lb_roc_auc_lst4), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду стал хуже, а также увеличился разброс. На валидационной выборке изменений практически нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4:__ Создать признаки на основе отношения: TransactionAmt к вычисленной статистике. Статистика - среднее значение / стандартное отклонение TransactionAmt, сгруппированное по card1 - card6, addr1, addr2, и по признакам, созданным в задании 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5 = train4.copy()\n",
    "col_lst = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'cards1', 'cards2', 'cards3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_lst:\n",
    "    mean_tamt = train5.groupby(col)['TransactionAmt'].mean()\n",
    "    std_tamt = train5.groupby(col)['TransactionAmt'].std()\n",
    "    train5[f\"{col}_TransactionAmt_mean\"] = train5['TransactionAmt'] / train5[col].map(mean_tamt)\n",
    "    train5[f\"{col}_TransactionAmt_std\"] = train5['TransactionAmt'] / train5[col].map(std_tamt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(415, 21, 'isFraud')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train5_num_features = train5.select_dtypes(\"number\").columns.to_list()\n",
    "train5_dum_features = train5.select_dtypes(\"object\").columns.to_list()\n",
    "train5_target = train5_num_features.pop(1)\n",
    "len(train5_num_features), len(train5_dum_features), train5_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = train5[train5_num_features + train5_dum_features].apply(le().fit_transform)\n",
    "y = train5[target]\n",
    "groups = train5['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df5 = lb_train_df4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_lst:\n",
    "    mean_tamt = lb_train_df5.groupby(col)['TransactionAmt'].mean()\n",
    "    std_tamt = lb_train_df5.groupby(col)['TransactionAmt'].std()\n",
    "    lb_train_df5[f\"{col}_TransactionAmt_mean\"] = lb_train_df5['TransactionAmt'] / lb_train_df5[col].map(mean_tamt)\n",
    "    lb_train_df5[f\"{col}_TransactionAmt_std\"] = lb_train_df5['TransactionAmt'] / lb_train_df5[col].map(std_tamt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train5 = lb_train_df5[train5_num_features + train5_dum_features].apply(le().fit_transform)\n",
    "dleaderboard5 = xgb.DMatrix(\n",
    "    data=lb_train5, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9258, valid-score = 0.8862\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9281, valid-score = 0.8809\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9237, valid-score = 0.8917\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9247, valid-score = 0.8949\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9257, valid-score = 0.89\n",
      "=====================================================================\n",
      "CV-results train: 0.9256 +/- 0.001\n",
      "CV-results valid: 0.8887 +/- 0.005\n",
      "OOF-score = 0.8889\n",
      "Wall time: 20min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators5, oof_score5, fold_train_scores5, fold_valid_scores5, oof_predictions5 = make_cross_validation(\n",
    "    data5, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8621245446452674, 0.8595120377482472, 0.8567764620046094, 0.8564390194425222, 0.8571764022800783]\n",
      "Leaderboard score 0.8584 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst5 = []\n",
    "for est in estimators5:\n",
    "    y_lb_pred = est.predict(dleaderboard5)\n",
    "    lb_roc_auc_lst5.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst5)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst5), 4),  '+/-', round(np.std(lb_roc_auc_lst5), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду улучшился до уровня, который был получен в задании 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 5:__ Создать признаки на основе отношения: D15 к вычисленной статистике. Статистика - среднее значение / стандартное отклонение D15, сгруппированное по card1 - card6, addr1, addr2, и по признакам, созданным в задании 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train6 = train5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48809"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#В признаке много пропусков, но обрабатывать пока не буду.\n",
    "train6.D15.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_lst:\n",
    "    mean_tamt = train6.groupby(col)['D15'].mean()\n",
    "    std_tamt = train6.groupby(col)['D15'].std()\n",
    "    train6[f\"{col}_D15_mean\"] = train6['D15'] / train6[col].map(mean_tamt)\n",
    "    train6[f\"{col}_D15_std\"] = train6['D15'] / train6[col].map(std_tamt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437, 21, 'isFraud')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train6_num_features = train6.select_dtypes(\"number\").columns.to_list()\n",
    "train6_dum_features = train6.select_dtypes(\"object\").columns.to_list()\n",
    "train6_target = train6_num_features.pop(1)\n",
    "len(train6_num_features), len(train6_dum_features), train6_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6 = train6[train6_num_features + train6_dum_features].apply(le().fit_transform)\n",
    "y = train6[target]\n",
    "groups = train6['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = ['card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'cards1', 'cards2', 'cards3']\n",
    "lb_train_df6 = lb_train_df5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in col_lst:\n",
    "    mean_tamt = lb_train_df6.groupby(col)['D15'].mean()\n",
    "    std_tamt = lb_train_df6.groupby(col)['D15'].std()\n",
    "    lb_train_df6[f\"{col}_D15_mean\"] = lb_train_df6['D15'] / lb_train_df6[col].map(mean_tamt)\n",
    "    lb_train_df6[f\"{col}_D15_std\"] = lb_train_df6['D15'] / lb_train_df6[col].map(std_tamt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train6 = lb_train_df6[train6_num_features + train6_dum_features].apply(le().fit_transform)\n",
    "dleaderboard6 = xgb.DMatrix(\n",
    "    data=lb_train6, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9256, valid-score = 0.8841\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.931, valid-score = 0.8811\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9248, valid-score = 0.8893\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.926, valid-score = 0.8954\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9253, valid-score = 0.8891\n",
      "=====================================================================\n",
      "CV-results train: 0.9265 +/- 0.002\n",
      "CV-results valid: 0.8878 +/- 0.005\n",
      "OOF-score = 0.8877\n",
      "Wall time: 25min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators6, oof_score6, fold_train_scores6, fold_valid_scores6, oof_predictions6 = make_cross_validation(\n",
    "    data6, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8618038059429548, 0.859794365088107, 0.8589981669013488, 0.8583505435694214, 0.858302258825314]\n",
      "Leaderboard score 0.8594 +/- 0.001\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst6 = []\n",
    "for est in estimators6:\n",
    "    y_lb_pred = est.predict(dleaderboard6)\n",
    "    lb_roc_auc_lst6.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst6)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst6), 4),  '+/-', round(np.std(lb_roc_auc_lst6), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду улучшился до уровня, который был получен в задании 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 6:__ выделить дробную часть и целую часть признака TransactionAmt в два отдельных признака. После создать отдельных признак - логарифм от TransactionAmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train7 = train6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train7['TransactionAmt_int'] = train7['TransactionAmt'].astype(int)\n",
    "train7['TransactionAmt_frac'] = train7['TransactionAmt'] - train7['TransactionAmt_int']\n",
    "train7['TransactionAmt_log'] = np.log(train7['TransactionAmt_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 21, 'isFraud')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train7_num_features = train7.select_dtypes(\"number\").columns.to_list()\n",
    "train7_dum_features = train7.select_dtypes(\"object\").columns.to_list()\n",
    "train7_target = train7_num_features.pop(1)\n",
    "len(train7_num_features), len(train7_dum_features), train7_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = train7[train7_num_features + train7_dum_features].apply(le().fit_transform)\n",
    "y = train7[target]\n",
    "groups = train7['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df7 = lb_train_df6.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df7['TransactionAmt_int'] = lb_train_df7['TransactionAmt'].astype(int)\n",
    "lb_train_df7['TransactionAmt_frac'] = lb_train_df7['TransactionAmt'] - lb_train_df7['TransactionAmt_int']\n",
    "lb_train_df7['TransactionAmt_log'] = np.log(lb_train_df7['TransactionAmt_int'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train7 = lb_train_df7[train7_num_features + train7_dum_features].apply(le().fit_transform)\n",
    "dleaderboard7 = xgb.DMatrix(\n",
    "    data=lb_train7, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9276, valid-score = 0.8859\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.93, valid-score = 0.8814\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9298, valid-score = 0.8902\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9248, valid-score = 0.8963\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9264, valid-score = 0.8895\n",
      "=====================================================================\n",
      "CV-results train: 0.9277 +/- 0.002\n",
      "CV-results valid: 0.8887 +/- 0.005\n",
      "OOF-score = 0.8886\n",
      "Wall time: 23min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators7, oof_score7, fold_train_scores7, fold_valid_scores7, oof_predictions7 = make_cross_validation(\n",
    "    data7, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8596013654813044, 0.8616407844697681, 0.8569071060003959, 0.8577171905942618, 0.857110774215552]\n",
      "Leaderboard score 0.8586 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst7 = []\n",
    "for est in estimators7:\n",
    "    y_lb_pred = est.predict(dleaderboard7)\n",
    "    lb_roc_auc_lst7.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst7)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst7), 4),  '+/-', round(np.std(lb_roc_auc_lst7), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду стал хуже. На валидационной выборке почти ничего не изменилось"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 7 (опция):__ выполнить предварительную подготовку / очистку признаков P_emaildomain и R_emaildomain (что и как делать - остается на ваше усмотрение) и сделать Frequency Encoding для очищенных признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Предварительную обработку этих признаков я уже сделал в ДЗ2, очистив от пустых значений, некорректных и выделив в отдельные признаки домен 1-го и 2-го уровня. Сюда сразу подтянул тот подготовленный датасет. Поэтому дальше сделаю Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train8 = train7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = ['P_emaildomain', 'R_emaildomain', 'P_emaildomain_zone', \n",
    "           'R_emaildomain_zone', 'P_emaildomain_domain', 'R_emaildomain_domain']\n",
    "for col in col_lst:\n",
    "    freq_encoder = train8[col].value_counts(normalize=True)\n",
    "    train8[f\"{col}_freq_enc\"] = train8[col].map(freq_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(446, 21, 'isFraud')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Разбиваем признаки по типам: числовые, нечисловые, цель\n",
    "train8_num_features = train8.select_dtypes(\"number\").columns.to_list()\n",
    "train8_dum_features = train8.select_dtypes(\"object\").columns.to_list()\n",
    "train8_target = train8_num_features.pop(1)\n",
    "len(train8_num_features), len(train8_dum_features), train8_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data8 = train8[train8_num_features + train8_dum_features].apply(le().fit_transform)\n",
    "y = train8[target]\n",
    "groups = train8['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Добавим новые признаки в датасет лидерборда:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train_df8 = lb_train_df7.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = ['P_emaildomain', 'R_emaildomain', 'P_emaildomain_zone', \n",
    "           'R_emaildomain_zone', 'P_emaildomain_domain', 'R_emaildomain_domain']\n",
    "for col in col_lst:\n",
    "    freq_encoder = lb_train_df8[col].value_counts(normalize=True)\n",
    "    lb_train_df8[f\"{col}_freq_enc\"] = lb_train_df8[col].map(freq_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_train8 = lb_train_df8[train8_num_features + train8_dum_features].apply(le().fit_transform)\n",
    "dleaderboard8 = xgb.DMatrix(\n",
    "    data=lb_train8, label=y_lb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Сделаем кросс-валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9278, valid-score = 0.886\n",
      "=====================================================================\n",
      "Fold: 2, train-observations = 143981, valid-observations = 35996\n",
      "train-score = 0.9321, valid-score = 0.8819\n",
      "=====================================================================\n",
      "Fold: 3, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9258, valid-score = 0.8924\n",
      "=====================================================================\n",
      "Fold: 4, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9268, valid-score = 0.8963\n",
      "=====================================================================\n",
      "Fold: 5, train-observations = 143982, valid-observations = 35995\n",
      "train-score = 0.9289, valid-score = 0.8895\n",
      "=====================================================================\n",
      "CV-results train: 0.9283 +/- 0.002\n",
      "CV-results valid: 0.8892 +/- 0.005\n",
      "OOF-score = 0.8895\n",
      "Wall time: 40min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "estimators8, oof_score8, fold_train_scores8, fold_valid_scores8, oof_predictions8 = make_cross_validation(\n",
    "    data8, y, xgb, metric=roc_auc_score, cv_strategy=cv_strategy, params=params, groups=groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8597559441610723, 0.8551579693515149, 0.8564882797740226, 0.8552636825803936, 0.8527500775632968]\n",
      "Leaderboard score 0.8559 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "lb_roc_auc_lst8 = []\n",
    "for est in estimators8:\n",
    "    y_lb_pred = est.predict(dleaderboard8)\n",
    "    lb_roc_auc_lst8.append(roc_auc_score(y_lb, y_lb_pred))\n",
    "print(lb_roc_auc_lst8)\n",
    "print('Leaderboard score', round(np.mean(lb_roc_auc_lst8), 4),  '+/-', round(np.std(lb_roc_auc_lst8), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Результат по лидерборду стал хуже. На валидационной выборке почти ничего не изменилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраняем датасеты для следующего ДЗ\n",
    "train = train8.to_csv(\"assignment2_data/train5.csv\", index=False)\n",
    "lb = lb_train_df8.to_csv(\"assignment2_data/lb5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
